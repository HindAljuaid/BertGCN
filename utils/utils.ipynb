{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTAtPWJ2rjK8"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
        "import sys\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index"
      ],
      "metadata": {
        "id": "4mTAldInrnJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)"
      ],
      "metadata": {
        "id": "G5lQl4CLro75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset_str):\n",
        "    \"\"\"\n",
        "    Loads input data from gcn/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
        "        object;\n",
        "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\n",
        "        \"data/ind.{}.test.index\".format(dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "\n",
        "    # training nodes are training docs, no initial features\n",
        "    # print(\"x: \", x)\n",
        "    # test nodes are training docs, no initial features\n",
        "    # print(\"tx: \", tx)\n",
        "    # both labeled and unlabeled training instances are training docs and words\n",
        "    # print(\"allx: \", allx)\n",
        "    # training labels are training doc labels\n",
        "    # print(\"y: \", y)\n",
        "    # test labels are test doc labels\n",
        "    # print(\"ty: \", ty)\n",
        "    # ally are labels for labels for allx, some will not have labels, i.e., all 0\n",
        "    # print(\"ally: \\n\")\n",
        "    # for i in ally:\n",
        "    # if(sum(i) == 0):\n",
        "    # print(i)\n",
        "    # graph edge weight is the word co-occurence or doc word frequency\n",
        "    # no need to build map, directly build csr_matrix\n",
        "    # print('graph : ', graph)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "        # Find isolated nodes, add them as zero-vecs into the right position\n",
        "        test_idx_range_full = range(\n",
        "            min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "    # print(len(labels))\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    # print(idx_test)\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
      ],
      "metadata": {
        "id": "_5tH6ckHr0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus(dataset_str):\n",
        "    \"\"\"\n",
        "    Loads input corpus from gcn/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
        "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    labels = np.vstack((ally, ty))\n",
        "    print(len(labels))\n",
        "\n",
        "    train_idx_orig = parse_index_file(\n",
        "        \"data/{}.train.index\".format(dataset_str))\n",
        "    train_size = len(train_idx_orig)\n",
        "\n",
        "    val_size = train_size - x.shape[0]\n",
        "    test_size = tx.shape[0]\n",
        "\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y) + val_size)\n",
        "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask, :] = labels[train_mask, :]\n",
        "    y_val[val_mask, :] = labels[val_mask, :]\n",
        "    y_test[test_mask, :] = labels[test_mask, :]\n",
        "\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size"
      ],
      "metadata": {
        "id": "1BRNMBZtr54W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx"
      ],
      "metadata": {
        "id": "vZN6OKxgr9hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)"
      ],
      "metadata": {
        "id": "jz22kX8fr_8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
      ],
      "metadata": {
        "id": "cc8qlylusCie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)"
      ],
      "metadata": {
        "id": "Hdv6BEfosE9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    \"\"\"Construct feed dictionary.\"\"\"\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i]\n",
        "                      for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict"
      ],
      "metadata": {
        "id": "uAxZbH5osHg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chebyshev_polynomials(adj, k):\n",
        "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (\n",
        "        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
        "\n",
        "    t_k = list()\n",
        "    t_k.append(sp.eye(adj.shape[0]))\n",
        "    t_k.append(scaled_laplacian)\n",
        "\n",
        "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
        "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
        "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
        "\n",
        "    return sparse_to_tuple(t_k)"
      ],
      "metadata": {
        "id": "TCxfu09XsK5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadWord2Vec(filename):\n",
        "    \"\"\"Read Word Vectors\"\"\"\n",
        "    vocab = []\n",
        "    embd = []\n",
        "    word_vector_map = {}\n",
        "    file = open(filename, 'r')\n",
        "    for line in file.readlines():\n",
        "        row = line.strip().split(' ')\n",
        "        if(len(row) > 2):\n",
        "            vocab.append(row[0])\n",
        "            vector = row[1:]\n",
        "            length = len(vector)\n",
        "            for i in range(length):\n",
        "                vector[i] = float(vector[i])\n",
        "            embd.append(vector)\n",
        "            word_vector_map[row[0]] = vector\n",
        "    print('Loaded Word Vectors!')\n",
        "    file.close()\n",
        "    return vocab, embd, word_vector_map"
      ],
      "metadata": {
        "id": "zW3EFd2AsNq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()"
      ],
      "metadata": {
        "id": "QDEMgESlsQe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}