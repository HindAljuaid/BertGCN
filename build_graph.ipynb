{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from utils import loadWord2Vec, clean_str\n",
        "from math import log\n",
        "from sklearn import svm\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sys\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "sAUlAGWVXBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(sys.argv) != 2:\n",
        "\tsys.exit(\"Use: python build_graph.py <dataset>\")\n",
        "\n",
        "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
        "# build corpus\n",
        "dataset = sys.argv[1]\n",
        "\n",
        "if dataset not in datasets:\n",
        "\tsys.exit(\"wrong dataset name\")"
      ],
      "metadata": {
        "id": "jOIp2jskXBy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Word Vectors\n",
        "# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n",
        "# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
        "#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "# word_embeddings_dim = len(embd[0])\n",
        "\n",
        "word_embeddings_dim = 300\n",
        "word_vector_map = {}"
      ],
      "metadata": {
        "id": "tnXFBsW1XFpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shulffing\n",
        "doc_name_list = []\n",
        "doc_train_list = []\n",
        "doc_test_list = []"
      ],
      "metadata": {
        "id": "BMATa600XTWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data/' + dataset + '.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "    doc_name_list.append(line.strip())\n",
        "    temp = line.split(\"\\t\")\n",
        "    if temp[1].find('test') != -1:\n",
        "        doc_test_list.append(line.strip())\n",
        "    elif temp[1].find('train') != -1:\n",
        "        doc_train_list.append(line.strip())\n",
        "f.close()\n",
        "# print(doc_train_list)\n",
        "# print(doc_test_list)"
      ],
      "metadata": {
        "id": "UR668JR5XWW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_content_list = []\n",
        "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "    doc_content_list.append(line.strip())\n",
        "f.close()\n",
        "# print(doc_content_list)"
      ],
      "metadata": {
        "id": "zUmjJhOiXZ3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = []\n",
        "for train_name in doc_train_list:\n",
        "    train_id = doc_name_list.index(train_name)\n",
        "    train_ids.append(train_id)\n",
        "print(train_ids)\n",
        "random.shuffle(train_ids)"
      ],
      "metadata": {
        "id": "goYtyZs-XgOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partial labeled data\n",
        "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
        "\n",
        "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
        "f = open('data/' + dataset + '.train.index', 'w')\n",
        "f.write(train_ids_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "yLiJx3LFXjKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids = []\n",
        "for test_name in doc_test_list:\n",
        "    test_id = doc_name_list.index(test_name)\n",
        "    test_ids.append(test_id)\n",
        "print(test_ids)\n",
        "random.shuffle(test_ids)"
      ],
      "metadata": {
        "id": "nhB7y2iOXqk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
        "f = open('data/' + dataset + '.test.index', 'w')\n",
        "f.write(test_ids_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "rwNiR76EXv4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = train_ids + test_ids\n",
        "print(ids)\n",
        "print(len(ids))"
      ],
      "metadata": {
        "id": "DNYwrBaGXzJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_doc_name_list = []\n",
        "shuffle_doc_words_list = []\n",
        "for id in ids:\n",
        "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
        "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
        "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
        "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)"
      ],
      "metadata": {
        "id": "YKy7eoH1X2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data/' + dataset + '_shuffle.txt', 'w')\n",
        "f.write(shuffle_doc_name_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "XuZD1wi0X4lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data/corpus/' + dataset + '_shuffle.txt', 'w')\n",
        "f.write(shuffle_doc_words_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "DQWzIAJ4X7Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocab\n",
        "word_freq = {}\n",
        "word_set = set()\n",
        "for doc_words in shuffle_doc_words_list:\n",
        "    words = doc_words.split()\n",
        "    for word in words:\n",
        "        word_set.add(word)\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "vocab = list(word_set)\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "wuV3isSrX_oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_doc_list = {}\n",
        "\n",
        "for i in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    appeared = set()\n",
        "    for word in words:\n",
        "        if word in appeared:\n",
        "            continue\n",
        "        if word in word_doc_list:\n",
        "            doc_list = word_doc_list[word]\n",
        "            doc_list.append(i)\n",
        "            word_doc_list[word] = doc_list\n",
        "        else:\n",
        "            word_doc_list[word] = [i]\n",
        "        appeared.add(word)"
      ],
      "metadata": {
        "id": "UtU8D8VMX_p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)"
      ],
      "metadata": {
        "id": "Q1JnARuVYKtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_id_map = {}\n",
        "for i in range(vocab_size):\n",
        "    word_id_map[vocab[i]] = i"
      ],
      "metadata": {
        "id": "f6odBAZsYaFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_str = '\\n'.join(vocab)\n",
        "\n",
        "f = open('data/corpus/' + dataset + '_vocab.txt', 'w')\n",
        "f.write(vocab_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "aIZt_OB4Yddw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Word definitions begin\n",
        "\n",
        "\n",
        "definitions = []\n",
        "\n",
        "for word in vocab:\n",
        "    word = word.strip()\n",
        "    synsets = wn.synsets(clean_str(word))\n",
        "    word_defs = []\n",
        "    for synset in synsets:\n",
        "        syn_def = synset.definition()\n",
        "        word_defs.append(syn_def)\n",
        "    word_des = ' '.join(word_defs)\n",
        "    if word_des == '':\n",
        "        word_des = '<PAD>'\n",
        "    definitions.append(word_des)\n",
        "\n",
        "string = '\\n'.join(definitions)"
      ],
      "metadata": {
        "id": "Pr3S4IB6Yzyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\n",
        "f.write(string)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "LMNOHvuqZDsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf_vec.fit_transform(definitions)\n",
        "tfidf_matrix_array = tfidf_matrix.toarray()\n",
        "print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))"
      ],
      "metadata": {
        "id": "56VTgFueZINn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = []\n",
        "\n",
        "for i in range(len(vocab)):\n",
        "    word = vocab[i]\n",
        "    vector = tfidf_matrix_array[i]\n",
        "    str_vector = []\n",
        "    for j in range(len(vector)):\n",
        "        str_vector.append(str(vector[j]))\n",
        "    temp = ' '.join(str_vector)\n",
        "    word_vector = word + ' ' + temp\n",
        "    word_vectors.append(word_vector)\n",
        "\n",
        "string = '\\n'.join(word_vectors)"
      ],
      "metadata": {
        "id": "lw3x_hoZZbX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\n",
        "f.write(string)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "4etIDCNiZh98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
        "_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
        "word_embeddings_dim = len(embd[0])\n",
        "\n",
        "##Word definitions end"
      ],
      "metadata": {
        "id": "SInzdUSoZmD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label list\n",
        "label_set = set()\n",
        "for doc_meta in shuffle_doc_name_list:\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label_set.add(temp[2])\n",
        "label_list = list(label_set)\n",
        "\n",
        "label_list_str = '\\n'.join(label_list)\n",
        "f = open('data/corpus/' + dataset + '_labels.txt', 'w')\n",
        "f.write(label_list_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "RlVfIJ2DZ7Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x: feature vectors of training docs, no initial features\n",
        "# slect 90% training set\n",
        "train_size = len(train_ids)\n",
        "val_size = int(0.1 * train_size)\n",
        "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
        "# different training rates"
      ],
      "metadata": {
        "id": "Ek7HHWXjZ_5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
        "real_train_doc_names_str = '\\n'.join(real_train_doc_names)"
      ],
      "metadata": {
        "id": "yhaAw5BnaDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data/' + dataset + '.real_train.name', 'w')\n",
        "f.write(real_train_doc_names_str)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "WKkyZyP7aGzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_x = []\n",
        "col_x = []\n",
        "data_x = []\n",
        "for i in range(real_train_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            # print(doc_vec)\n",
        "            # print(np.array(word_vector))\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_x.append(i)\n",
        "        col_x.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len"
      ],
      "metadata": {
        "id": "x_0Y2t82aNTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
        "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
        "    real_train_size, word_embeddings_dim))"
      ],
      "metadata": {
        "id": "LcacdyGVaQNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "for i in range(real_train_size):\n",
        "    doc_meta = shuffle_doc_name_list[i]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    y.append(one_hot)\n",
        "y = np.array(y)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "YFs_eQeQaSzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tx: feature vectors of test docs, no initial features\n",
        "test_size = len(test_ids)"
      ],
      "metadata": {
        "id": "czmKGYvxaVUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_tx = []\n",
        "col_tx = []\n",
        "data_tx = []\n",
        "for i in range(test_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i + train_size]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_tx.append(i)\n",
        "        col_tx.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len"
      ],
      "metadata": {
        "id": "2AV0VXJeaZAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
        "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
        "                   shape=(test_size, word_embeddings_dim))"
      ],
      "metadata": {
        "id": "kklvH7qLabjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ty = []\n",
        "for i in range(test_size):\n",
        "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ty.append(one_hot)\n",
        "ty = np.array(ty)\n",
        "print(ty)"
      ],
      "metadata": {
        "id": "EfXDDil_aepr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
        "# (a superset of x)\n",
        "# unlabeled training instances -> words\n",
        "\n",
        "word_vectors = np.random.uniform(-0.01, 0.01,\n",
        "                                 (vocab_size, word_embeddings_dim))"
      ],
      "metadata": {
        "id": "YTulrWo4ao8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(vocab)):\n",
        "    word = vocab[i]\n",
        "    if word in word_vector_map:\n",
        "        vector = word_vector_map[word]\n",
        "        word_vectors[i] = vector"
      ],
      "metadata": {
        "id": "OPrGTQcCasXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_allx = []\n",
        "col_allx = []\n",
        "data_allx = []\n",
        "\n",
        "for i in range(train_size):\n",
        "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_len = len(words)\n",
        "    for word in words:\n",
        "        if word in word_vector_map:\n",
        "            word_vector = word_vector_map[word]\n",
        "            doc_vec = doc_vec + np.array(word_vector)\n",
        "\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_allx.append(int(i))\n",
        "        col_allx.append(j)\n",
        "        # np.random.uniform(-0.25, 0.25)\n",
        "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
        "for i in range(vocab_size):\n",
        "    for j in range(word_embeddings_dim):\n",
        "        row_allx.append(int(i + train_size))\n",
        "        col_allx.append(j)\n",
        "        data_allx.append(word_vectors.item((i, j)))\n",
        "\n",
        "\n",
        "row_allx = np.array(row_allx)\n",
        "col_allx = np.array(col_allx)\n",
        "data_allx = np.array(data_allx)\n",
        "\n",
        "allx = sp.csr_matrix(\n",
        "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))"
      ],
      "metadata": {
        "id": "Fd5e4I5TbBis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ally = []\n",
        "for i in range(train_size):\n",
        "    doc_meta = shuffle_doc_name_list[i]\n",
        "    temp = doc_meta.split('\\t')\n",
        "    label = temp[2]\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    label_index = label_list.index(label)\n",
        "    one_hot[label_index] = 1\n",
        "    ally.append(one_hot)\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    one_hot = [0 for l in range(len(label_list))]\n",
        "    ally.append(one_hot)\n",
        "\n",
        "ally = np.array(ally)\n",
        "\n",
        "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)"
      ],
      "metadata": {
        "id": "AzMuVabtbLVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Doc word heterogeneous graph\n",
        "\n",
        "\n",
        "# word co-occurence with context windows\n",
        "window_size = 20\n",
        "windows = []\n",
        "\n",
        "for doc_words in shuffle_doc_words_list:\n",
        "    words = doc_words.split()\n",
        "    length = len(words)\n",
        "    if length <= window_size:\n",
        "        windows.append(words)\n",
        "    else:\n",
        "        # print(length, length - window_size + 1)\n",
        "        for j in range(length - window_size + 1):\n",
        "            window = words[j: j + window_size]\n",
        "            windows.append(window)\n",
        "            # print(window)"
      ],
      "metadata": {
        "id": "aghnIbCdbVCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_window_freq = {}\n",
        "for window in windows:\n",
        "    appeared = set()\n",
        "    for i in range(len(window)):\n",
        "        if window[i] in appeared:\n",
        "            continue\n",
        "        if window[i] in word_window_freq:\n",
        "            word_window_freq[window[i]] += 1\n",
        "        else:\n",
        "            word_window_freq[window[i]] = 1\n",
        "        appeared.add(window[i])"
      ],
      "metadata": {
        "id": "_F3DX3bMbaA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pair_count = {}\n",
        "for window in windows:\n",
        "    for i in range(1, len(window)):\n",
        "        for j in range(0, i):\n",
        "            word_i = window[i]\n",
        "            word_i_id = word_id_map[word_i]\n",
        "            word_j = window[j]\n",
        "            word_j_id = word_id_map[word_j]\n",
        "            if word_i_id == word_j_id:\n",
        "                continue\n",
        "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1\n",
        "            # two orders\n",
        "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "            if word_pair_str in word_pair_count:\n",
        "                word_pair_count[word_pair_str] += 1\n",
        "            else:\n",
        "                word_pair_count[word_pair_str] = 1"
      ],
      "metadata": {
        "id": "si82q6mabeDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = []\n",
        "col = []\n",
        "weight = []\n",
        "\n",
        "# pmi as weights\n",
        "\n",
        "num_window = len(windows)\n",
        "\n",
        "for key in word_pair_count:\n",
        "    temp = key.split(',')\n",
        "    i = int(temp[0])\n",
        "    j = int(temp[1])\n",
        "    count = word_pair_count[key]\n",
        "    word_freq_i = word_window_freq[vocab[i]]\n",
        "    word_freq_j = word_window_freq[vocab[j]]\n",
        "    pmi = log((1.0 * count / num_window) /\n",
        "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
        "    if pmi <= 0:\n",
        "        continue\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)"
      ],
      "metadata": {
        "id": "RPr0nGambq0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word vector cosine similarity as weights\n",
        "\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    for j in range(vocab_size):\n",
        "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
        "            vector_i = np.array(word_vector_map[vocab[i]])\n",
        "            vector_j = np.array(word_vector_map[vocab[j]])\n",
        "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
        "            if similarity > 0.9:\n",
        "                print(vocab[i], vocab[j], similarity)\n",
        "                row.append(train_size + i)\n",
        "                col.append(train_size + j)\n",
        "                weight.append(similarity)\n"
      ],
      "metadata": {
        "id": "-aBTbLS7bv1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc word frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[doc_id]\n",
        "    words = doc_words.split()\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1"
      ],
      "metadata": {
        "id": "K6FzQlNVb1Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(shuffle_doc_words_list)):\n",
        "    doc_words = shuffle_doc_words_list[i]\n",
        "    words = doc_words.split()\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_size)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
        "                  word_doc_freq[vocab[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)"
      ],
      "metadata": {
        "id": "IUHEUWXJb7Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_size = train_size + vocab_size + test_size\n",
        "adj = sp.csr_matrix(\n",
        "    (weight, (row, col)), shape=(node_size, node_size))"
      ],
      "metadata": {
        "id": "3wmVql07b95l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dump objects\n",
        "f = open(\"data/ind.{}.x\".format(dataset), 'wb')\n",
        "pkl.dump(x, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "M0nMnazFcCXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"data/ind.{}.y\".format(dataset), 'wb')\n",
        "pkl.dump(y, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "n5zyk7GmcEHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"data/ind.{}.tx\".format(dataset), 'wb')\n",
        "pkl.dump(tx, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "w5aH4YrncLkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"data/ind.{}.ty\".format(dataset), 'wb')\n",
        "pkl.dump(ty, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "6kMcGk54cP5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"data/ind.{}.allx\".format(dataset), 'wb')\n",
        "pkl.dump(allx, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "FLUARebAcR0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"data/ind.{}.ally\".format(dataset), 'wb')\n",
        "pkl.dump(ally, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "7gZjLXiFcTm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"data/ind.{}.adj\".format(dataset), 'wb')\n",
        "pkl.dump(adj, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "6Q3-jLFQcVce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}