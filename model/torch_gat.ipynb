{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clL6a2MqqmM3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Graph Attention Networks in DGL using SPMV optimization.\n",
        "References\n",
        "----------\n",
        "Paper: https://arxiv.org/abs/1710.10903\n",
        "Author's code: https://github.com/PetarV-/GAT\n",
        "Pytorch implementation: https://github.com/Diego999/pyGAT\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "from dgl.nn import GATConv"
      ],
      "metadata": {
        "id": "D01vxdXmqrZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop=0,\n",
        "                 attn_drop=0,\n",
        "                 negative_slope=0.2,\n",
        "                 residual=False\n",
        "    ):\n",
        "        super(GAT, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, inputs, g):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](g, h).mean(1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "9i1bERkeqvhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}