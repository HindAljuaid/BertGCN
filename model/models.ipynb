{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mm6jOmUqHF9"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from .torch_gcn import GCN\n",
        "from .torch_gat import GAT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(th.nn.Module):\n",
        "    def __init__(self, pretrained_model='roberta_base', nb_class=20):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.nb_class = nb_class\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
        "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
        "        self.classifier = th.nn.Linear(self.feat_dim, nb_class)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
        "        cls_logit = self.classifier(cls_feats)\n",
        "        return cls_logit"
      ],
      "metadata": {
        "id": "-KqS36BCqNT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertGCN(th.nn.Module):\n",
        "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, n_hidden=200, dropout=0.5):\n",
        "        super(BertGCN, self).__init__()\n",
        "        self.m = m\n",
        "        self.nb_class = nb_class\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
        "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
        "        self.classifier = th.nn.Linear(self.feat_dim, nb_class)\n",
        "        self.gcn = GCN(\n",
        "            in_feats=self.feat_dim,\n",
        "            n_hidden=n_hidden,\n",
        "            n_classes=nb_class,\n",
        "            n_layers=gcn_layers-1,\n",
        "            activation=F.elu,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, g, idx):\n",
        "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
        "        if self.training:\n",
        "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
        "            g.ndata['cls_feats'][idx] = cls_feats\n",
        "        else:\n",
        "            cls_feats = g.ndata['cls_feats'][idx]\n",
        "        cls_logit = self.classifier(cls_feats)\n",
        "        cls_pred = th.nn.Softmax(dim=1)(cls_logit)\n",
        "        gcn_logit = self.gcn(g.ndata['cls_feats'], g, g.edata['edge_weight'])[idx]\n",
        "        gcn_pred = th.nn.Softmax(dim=1)(gcn_logit)\n",
        "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
        "        pred = th.log(pred)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "qYbUuX9-qRkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertGAT(th.nn.Module):\n",
        "    def __init__(self, pretrained_model='roberta_base', nb_class=20, m=0.7, gcn_layers=2, heads=8, n_hidden=32, dropout=0.5):\n",
        "        super(BertGAT, self).__init__()\n",
        "        self.m = m\n",
        "        self.nb_class = nb_class\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "        self.bert_model = AutoModel.from_pretrained(pretrained_model)\n",
        "        self.feat_dim = list(self.bert_model.modules())[-2].out_features\n",
        "        self.classifier = th.nn.Linear(self.feat_dim, nb_class)\n",
        "        self.gcn = GAT(\n",
        "                 num_layers=gcn_layers-1,\n",
        "                 in_dim=self.feat_dim,\n",
        "                 num_hidden=n_hidden,\n",
        "                 num_classes=nb_class,\n",
        "                 heads=[heads] * (gcn_layers-1) + [1],\n",
        "                 activation=F.elu,\n",
        "                 feat_drop=dropout,\n",
        "                 attn_drop=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, g, idx):\n",
        "        input_ids, attention_mask = g.ndata['input_ids'][idx], g.ndata['attention_mask'][idx]\n",
        "        if self.training:\n",
        "            cls_feats = self.bert_model(input_ids, attention_mask)[0][:, 0]\n",
        "            g.ndata['cls_feats'][idx] = cls_feats\n",
        "        else:\n",
        "            cls_feats = g.ndata['cls_feats'][idx]\n",
        "        cls_logit = self.classifier(cls_feats)\n",
        "        cls_pred = th.nn.Softmax(dim=1)(cls_logit)\n",
        "        gcn_logit = self.gcn(g.ndata['cls_feats'], g)[idx]\n",
        "        gcn_pred = th.nn.Softmax(dim=1)(gcn_logit)\n",
        "        pred = (gcn_pred+1e-10) * self.m + cls_pred * (1 - self.m)\n",
        "        pred = th.log(pred)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "7tCsdD_lqUdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}