{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VButTkMnpd7G"
      },
      "outputs": [],
      "source": [
        "from dgl.nn.pytorch import GraphConv\n",
        "from dgl import function as fn\n",
        "from dgl.base import DGLError\n",
        "from dgl.utils import expand_as_pair\n",
        "import torch as th"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvEdgeWeight(GraphConv):\n",
        "\n",
        "    def forward(self, graph, feat,  weight=None, edge_weights=None):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
        "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
        "            if self._norm == 'both':\n",
        "                degs = graph.out_degrees().float().clamp(min=1)\n",
        "                norm = th.pow(degs, -0.5)\n",
        "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                feat_src = feat_src * norm\n",
        "\n",
        "            if weight is not None:\n",
        "                if self.weight is not None:\n",
        "                    raise DGLError('External weight is provided while at the same time the'\n",
        "                                   ' module has defined its own weight parameter. Please'\n",
        "                                   ' create the module with flag weight=False.')\n",
        "            else:\n",
        "                weight = self.weight\n",
        "\n",
        "            if self._in_feats > self._out_feats:\n",
        "                # mult W first to reduce the feature size for aggregation.\n",
        "                if weight is not None:\n",
        "                    feat_src = th.matmul(feat_src, weight)\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                if edge_weights is None:\n",
        "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                else:\n",
        "                    graph.edata['a'] = edge_weights\n",
        "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "            else:\n",
        "                # aggregate first then mult W\n",
        "                graph.srcdata['h'] = feat_src\n",
        "                if edge_weights is None:\n",
        "                    graph.update_all(fn.copy_src(src='h', out='m'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                else:\n",
        "                    graph.edata['a'] = edge_weights\n",
        "                    graph.update_all(fn.u_mul_e('h', 'a', 'm'),\n",
        "                                     fn.sum(msg='m', out='h'))\n",
        "                rst = graph.dstdata['h']\n",
        "                if weight is not None:\n",
        "                    rst = th.matmul(rst, weight)\n",
        "\n",
        "            if self._norm != 'none':\n",
        "                degs = graph.in_degrees().float().clamp(min=1)\n",
        "                if self._norm == 'both':\n",
        "                    norm = th.pow(degs, -0.5)\n",
        "                else:\n",
        "                    norm = 1.0 / degs\n",
        "                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
        "                norm = th.reshape(norm, shp)\n",
        "                rst = rst * norm\n",
        "\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias\n",
        "\n",
        "            if self._activation is not None:\n",
        "                rst = self._activation(rst)\n",
        "\n",
        "            return rst"
      ],
      "metadata": {
        "id": "AE_pjYnCpntZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}