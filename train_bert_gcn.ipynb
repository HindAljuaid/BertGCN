{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRBk1KEBmmIx"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "from utils import *\n",
        "import dgl\n",
        "import torch.utils.data as Data\n",
        "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import argparse\n",
        "import sys\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from torch.optim import lr_scheduler\n",
        "from model import BertGCN, BertGAT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--max_length', type=int, default=128, help='the input length for bert')\n",
        "parser.add_argument('--batch_size', type=int, default=64)\n",
        "parser.add_argument('-m', '--m', type=float, default=0.7, help='the factor balancing BERT and GCN prediction')\n",
        "parser.add_argument('--nb_epochs', type=int, default=50)\n",
        "parser.add_argument('--bert_init', type=str, default='roberta-base',\n",
        "                    choices=['roberta-base', 'roberta-large', 'bert-base-uncased', 'bert-large-uncased'])\n",
        "parser.add_argument('--pretrained_bert_ckpt', default=None)\n",
        "parser.add_argument('--dataset', default='20ng', choices=['20ng', 'R8', 'R52', 'ohsumed', 'mr'])\n",
        "parser.add_argument('--checkpoint_dir', default=None, help='checkpoint directory, [bert_init]_[gcn_model]_[dataset] if not specified')\n",
        "parser.add_argument('--gcn_model', type=str, default='gcn', choices=['gcn', 'gat'])\n",
        "parser.add_argument('--gcn_layers', type=int, default=2)\n",
        "parser.add_argument('--n_hidden', type=int, default=200, help='the dimension of gcn hidden layer, the dimension for gat is n_hidden * heads')\n",
        "parser.add_argument('--heads', type=int, default=8, help='the number of attentionn heads for gat')\n",
        "parser.add_argument('--dropout', type=float, default=0.5)\n",
        "parser.add_argument('--gcn_lr', type=float, default=1e-3)\n",
        "parser.add_argument('--bert_lr', type=float, default=1e-5)"
      ],
      "metadata": {
        "id": "V_JNaMw5m7KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args()\n",
        "max_length = args.max_length\n",
        "batch_size = args.batch_size\n",
        "m = args.m\n",
        "nb_epochs = args.nb_epochs\n",
        "bert_init = args.bert_init\n",
        "pretrained_bert_ckpt = args.pretrained_bert_ckpt\n",
        "dataset = args.dataset\n",
        "checkpoint_dir = args.checkpoint_dir\n",
        "gcn_model = args.gcn_model\n",
        "gcn_layers = args.gcn_layers\n",
        "n_hidden = args.n_hidden\n",
        "heads = args.heads\n",
        "dropout = args.dropout\n",
        "gcn_lr = args.gcn_lr\n",
        "bert_lr = args.bert_lr"
      ],
      "metadata": {
        "id": "sfpgDlJsnJW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if checkpoint_dir is None:\n",
        "    ckpt_dir = './checkpoint/{}_{}_{}'.format(bert_init, gcn_model, dataset)\n",
        "else:\n",
        "    ckpt_dir = checkpoint_dir\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "shutil.copy(os.path.basename(__file__), ckpt_dir)"
      ],
      "metadata": {
        "id": "TjuMUHW2nOIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sh = logging.StreamHandler(sys.stdout)\n",
        "sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "sh.setLevel(logging.INFO)\n",
        "fh = logging.FileHandler(filename=os.path.join(ckpt_dir, 'training.log'), mode='w')\n",
        "fh.setFormatter(logging.Formatter('%(message)s'))\n",
        "fh.setLevel(logging.INFO)\n",
        "logger = logging.getLogger('training logger')\n",
        "logger.addHandler(sh)\n",
        "logger.addHandler(fh)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "BsOTShjknVqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpu = th.device('cpu')\n",
        "gpu = th.device('cuda:0')"
      ],
      "metadata": {
        "id": "ZVwM9tDhnZqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('arguments:')\n",
        "logger.info(str(args))\n",
        "logger.info('checkpoints will be saved in {}'.format(ckpt_dir))\n",
        "# Model"
      ],
      "metadata": {
        "id": "Wwu3UHcDne-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocess\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
        "\n",
        "adj: n*n sparse adjacency matrix\n",
        "y_train, y_val, y_test: n*c matrices \n",
        "train_mask, val_mask, test_mask: n-d bool array\n"
      ],
      "metadata": {
        "id": "jSg2YGaonhHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute number of real train/val/test/word nodes and number of classes\n",
        "nb_node = features.shape[0]\n",
        "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
        "nb_word = nb_node - nb_train - nb_val - nb_test\n",
        "nb_class = y_train.shape[1]"
      ],
      "metadata": {
        "id": "Mi_EaLCynk82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model according to class number\n",
        "if gcn_model == 'gcn':\n",
        "    model = BertGCN(nb_class=nb_class, pretrained_model=bert_init, m=m, gcn_layers=gcn_layers,\n",
        "                    n_hidden=n_hidden, dropout=dropout)\n",
        "else:\n",
        "    model = BertGAT(nb_class=nb_class, pretrained_model=bert_init, m=m, gcn_layers=gcn_layers,\n",
        "                    heads=heads, n_hidden=n_hidden, dropout=dropout)"
      ],
      "metadata": {
        "id": "t_e5_HFjnq3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if pretrained_bert_ckpt is not None:\n",
        "    ckpt = th.load(pretrained_bert_ckpt, map_location=gpu)\n",
        "    model.bert_model.load_state_dict(ckpt['bert_model'])\n",
        "    model.classifier.load_state_dict(ckpt['classifier'])"
      ],
      "metadata": {
        "id": "16YtBJnXntX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents and compute input encodings\n",
        "corpse_file = './data/corpus/' + dataset +'_shuffle.txt'\n",
        "with open(corpse_file, 'r') as f:\n",
        "    text = f.read()\n",
        "    text = text.replace('\\\\', '')\n",
        "    text = text.split('\\n')"
      ],
      "metadata": {
        "id": "ntbxLf-inxIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input(text, tokenizer):\n",
        "    input = tokenizer(text, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "#     print(input.keys())\n",
        "    return input.input_ids, input.attention_mask"
      ],
      "metadata": {
        "id": "G35ITU4Kn1T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask = encode_input(text, model.tokenizer)\n",
        "input_ids = th.cat([input_ids[:-nb_test], th.zeros((nb_word, max_length), dtype=th.long), input_ids[-nb_test:]])\n",
        "attention_mask = th.cat([attention_mask[:-nb_test], th.zeros((nb_word, max_length), dtype=th.long), attention_mask[-nb_test:]])"
      ],
      "metadata": {
        "id": "gdxBDByJn4xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform one-hot label to class ID for pytorch computation\n",
        "y = y_train + y_test + y_val\n",
        "y_train = y_train.argmax(axis=1)\n",
        "y = y.argmax(axis=1)"
      ],
      "metadata": {
        "id": "8YhFCpFGn9B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document mask used for update feature\n",
        "doc_mask  = train_mask + val_mask + test_mask"
      ],
      "metadata": {
        "id": "mrHQosxZn_nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build DGL Graph\n",
        "adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "g = dgl.from_scipy(adj_norm.astype('float32'), eweight_name='edge_weight')\n",
        "g.ndata['input_ids'], g.ndata['attention_mask'] = input_ids, attention_mask\n",
        "g.ndata['label'], g.ndata['train'], g.ndata['val'], g.ndata['test'] = \\\n",
        "    th.LongTensor(y), th.FloatTensor(train_mask), th.FloatTensor(val_mask), th.FloatTensor(test_mask)\n",
        "g.ndata['label_train'] = th.LongTensor(y_train)\n",
        "g.ndata['cls_feats'] = th.zeros((nb_node, model.feat_dim))\n",
        "\n",
        "logger.info('graph information:')\n",
        "logger.info(str(g))"
      ],
      "metadata": {
        "id": "VIoKsJh-oCGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create index loader\n",
        "train_idx = Data.TensorDataset(th.arange(0, nb_train, dtype=th.long))\n",
        "val_idx = Data.TensorDataset(th.arange(nb_train, nb_train + nb_val, dtype=th.long))\n",
        "test_idx = Data.TensorDataset(th.arange(nb_node-nb_test, nb_node, dtype=th.long))\n",
        "doc_idx = Data.ConcatDataset([train_idx, val_idx, test_idx])\n",
        "\n",
        "idx_loader_train = Data.DataLoader(train_idx, batch_size=batch_size, shuffle=True)\n",
        "idx_loader_val = Data.DataLoader(val_idx, batch_size=batch_size)\n",
        "idx_loader_test = Data.DataLoader(test_idx, batch_size=batch_size)\n",
        "idx_loader = Data.DataLoader(doc_idx, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "qUrvJuB7oGFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def update_feature():\n",
        "    global model, g, doc_mask\n",
        "    # no gradient needed, uses a large batchsize to speed up the process\n",
        "    dataloader = Data.DataLoader(\n",
        "        Data.TensorDataset(g.ndata['input_ids'][doc_mask], g.ndata['attention_mask'][doc_mask]),\n",
        "        batch_size=1024\n",
        "    )\n",
        "    with th.no_grad():\n",
        "        model = model.to(gpu)\n",
        "        model.eval()\n",
        "        cls_list = []\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids, attention_mask = [x.to(gpu) for x in batch]\n",
        "            output = model.bert_model(input_ids=input_ids, attention_mask=attention_mask)[0][:, 0]\n",
        "            cls_list.append(output.cpu())\n",
        "        cls_feat = th.cat(cls_list, axis=0)\n",
        "    g = g.to(cpu)\n",
        "    g.ndata['cls_feats'][doc_mask] = cls_feat\n",
        "    return g"
      ],
      "metadata": {
        "id": "poDwXZbooQ1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = th.optim.Adam([\n",
        "        {'params': model.bert_model.parameters(), 'lr': bert_lr},\n",
        "        {'params': model.classifier.parameters(), 'lr': bert_lr},\n",
        "        {'params': model.gcn.parameters(), 'lr': gcn_lr},\n",
        "    ], lr=1e-3\n",
        ")\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)"
      ],
      "metadata": {
        "id": "lzvZ9RenoTJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(engine, batch):\n",
        "    global model, g, optimizer\n",
        "    model.train()\n",
        "    model = model.to(gpu)\n",
        "    g = g.to(gpu)\n",
        "    optimizer.zero_grad()\n",
        "    (idx, ) = [x.to(gpu) for x in batch]\n",
        "    optimizer.zero_grad()\n",
        "    train_mask = g.ndata['train'][idx].type(th.BoolTensor)\n",
        "    y_pred = model(g, idx)[train_mask]\n",
        "    y_true = g.ndata['label_train'][idx][train_mask]\n",
        "    loss = F.nll_loss(y_pred, y_true)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    g.ndata['cls_feats'].detach_()\n",
        "    train_loss = loss.item()\n",
        "    with th.no_grad():\n",
        "        if train_mask.sum() > 0:\n",
        "            y_true = y_true.detach().cpu()\n",
        "            y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
        "            train_acc = accuracy_score(y_true, y_pred)\n",
        "        else:\n",
        "            train_acc = 1\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "trainer = Engine(train_step)"
      ],
      "metadata": {
        "id": "IZGk3WdRoecH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def reset_graph(trainer):\n",
        "    scheduler.step()\n",
        "    update_feature()\n",
        "    th.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def test_step(engine, batch):\n",
        "    global model, g\n",
        "    with th.no_grad():\n",
        "        model.eval()\n",
        "        model = model.to(gpu)\n",
        "        g = g.to(gpu)\n",
        "        (idx, ) = [x.to(gpu) for x in batch]\n",
        "        y_pred = model(g, idx)\n",
        "        y_true = g.ndata['label'][idx]\n",
        "        return y_pred, y_true\n",
        "\n",
        "\n",
        "evaluator = Engine(test_step)\n",
        "metrics={\n",
        "    'acc': Accuracy(),\n",
        "    'nll': Loss(th.nn.NLLLoss())\n",
        "}\n",
        "for n, f in metrics.items():\n",
        "    f.attach(evaluator, n)"
      ],
      "metadata": {
        "id": "ofwXQlgTokL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_training_results(trainer):\n",
        "    evaluator.run(idx_loader_train)\n",
        "    metrics = evaluator.state.metrics\n",
        "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    evaluator.run(idx_loader_val)\n",
        "    metrics = evaluator.state.metrics\n",
        "    val_acc, val_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    evaluator.run(idx_loader_test)\n",
        "    metrics = evaluator.state.metrics\n",
        "    test_acc, test_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    logger.info(\n",
        "        \"Epoch: {}  Train acc: {:.4f} loss: {:.4f}  Val acc: {:.4f} loss: {:.4f}  Test acc: {:.4f} loss: {:.4f}\"\n",
        "        .format(trainer.state.epoch, train_acc, train_nll, val_acc, val_nll, test_acc, test_nll)\n",
        "    )\n",
        "    if val_acc > log_training_results.best_val_acc:\n",
        "        logger.info(\"New checkpoint\")\n",
        "        th.save(\n",
        "            {\n",
        "                'bert_model': model.bert_model.state_dict(),\n",
        "                'classifier': model.classifier.state_dict(),\n",
        "                'gcn': model.gcn.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': trainer.state.epoch,\n",
        "            },\n",
        "            os.path.join(\n",
        "                ckpt_dir, 'checkpoint.pth'\n",
        "            )\n",
        "        )\n",
        "        log_training_results.best_val_acc = val_acc\n",
        "\n",
        "\n",
        "log_training_results.best_val_acc = 0\n",
        "g = update_feature()\n",
        "trainer.run(idx_loader, max_epochs=nb_epochs)"
      ],
      "metadata": {
        "id": "PIgkaCSUooyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}