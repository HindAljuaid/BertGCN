{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm2P15FBc78x"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "from utils import *\n",
        "import dgl\n",
        "import torch.utils.data as Data\n",
        "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer, Engine\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "import argparse, shutil, logging\n",
        "from torch.optim import lr_scheduler\n",
        "from model import BertClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--max_length', type=int, default=128, help='the input length for bert')\n",
        "parser.add_argument('--batch_size', type=int, default=128)\n",
        "parser.add_argument('--nb_epochs', type=int, default=60)\n",
        "parser.add_argument('--bert_lr', type=float, default=1e-4)\n",
        "parser.add_argument('--dataset', default='20ng', choices=['20ng', 'R8', 'R52', 'ohsumed', 'mr'])\n",
        "parser.add_argument('--bert_init', type=str, default='roberta-base',\n",
        "                    choices=['roberta-base', 'roberta-large', 'bert-base-uncased', 'bert-large-uncased'])\n",
        "parser.add_argument('--checkpoint_dir', default=None, help='checkpoint directory, [bert_init]_[dataset] if not specified')"
      ],
      "metadata": {
        "id": "7MgAWeaoeSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args()\n",
        "\n",
        "max_length = args.max_length\n",
        "batch_size = args.batch_size\n",
        "nb_epochs = args.nb_epochs\n",
        "bert_lr = args.bert_lr\n",
        "dataset = args.dataset\n",
        "bert_init = args.bert_init\n",
        "checkpoint_dir = args.checkpoint_dir\n",
        "if checkpoint_dir is None:\n",
        "    ckpt_dir = './checkpoint/{}_{}'.format(bert_init, dataset)\n",
        "else:\n",
        "    ckpt_dir = checkpoint_dir\n",
        "\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "shutil.copy(os.path.basename(__file__), ckpt_dir)"
      ],
      "metadata": {
        "id": "3UgFpWyMean5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sh = logging.StreamHandler(sys.stdout)\n",
        "sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "sh.setLevel(logging.INFO)\n",
        "fh = logging.FileHandler(filename=os.path.join(ckpt_dir, 'training.log'), mode='w')\n",
        "fh.setFormatter(logging.Formatter('%(message)s'))\n",
        "fh.setLevel(logging.INFO)\n",
        "logger = logging.getLogger('training logger')\n",
        "logger.addHandler(sh)\n",
        "logger.addHandler(fh)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "GD2blwPQefo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpu = th.device('cpu')\n",
        "gpu = th.device('cuda:0')"
      ],
      "metadata": {
        "id": "drAAq1-beias"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info('arguments:')\n",
        "logger.info(str(args))\n",
        "logger.info('checkpoints will be saved in {}'.format(ckpt_dir))"
      ],
      "metadata": {
        "id": "2kV9wIv7ekxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocess\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(dataset)\n",
        "\n",
        "y_train, y_val, y_test: n*c matrices \n",
        "train_mask, val_mask, test_mask: n-d bool array\n",
        "train_size, test_size: unused\n"
      ],
      "metadata": {
        "id": "xkah7ustequG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute number of real train/val/test/word nodes and number of classes\n",
        "nb_node = adj.shape[0]\n",
        "nb_train, nb_val, nb_test = train_mask.sum(), val_mask.sum(), test_mask.sum()\n",
        "nb_word = nb_node - nb_train - nb_val - nb_test\n",
        "nb_class = y_train.shape[1]"
      ],
      "metadata": {
        "id": "njBciz8xewAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model according to class number\n",
        "model = BertClassifier(pretrained_model=bert_init, nb_class=nb_class)"
      ],
      "metadata": {
        "id": "8bFdS1OReyRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform one-hot label to class ID for pytorch computation\n",
        "y = th.LongTensor((y_train + y_val +y_test).argmax(axis=1))\n",
        "label = {}\n",
        "label['train'], label['val'], label['test'] = y[:nb_train], y[nb_train:nb_train+nb_val], y[-nb_test:]"
      ],
      "metadata": {
        "id": "yNUo3GAle1fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents and compute input encodings\n",
        "corpus_file = './data/corpus/'+dataset+'_shuffle.txt'\n",
        "with open(corpus_file, 'r') as f:\n",
        "    text = f.read()\n",
        "    text=text.replace('\\\\', '')\n",
        "    text = text.split('\\n')"
      ],
      "metadata": {
        "id": "DV-JEDPUe6El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input(text, tokenizer):\n",
        "    input = tokenizer(text, max_length=max_length, truncation=True, padding=True, return_tensors='pt')\n",
        "    return input.input_ids, input.attention_mask\n",
        "\n",
        "input_ids, attention_mask = {}, {}\n",
        "\n",
        "input_ids_, attention_mask_ = encode_input(text, model.tokenizer)"
      ],
      "metadata": {
        "id": "cqP7LRv3fBJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train/test/val datasets and dataloaders\n",
        "input_ids['train'], input_ids['val'], input_ids['test'] =  input_ids_[:nb_train], input_ids_[nb_train:nb_train+nb_val], input_ids_[-nb_test:]\n",
        "attention_mask['train'], attention_mask['val'], attention_mask['test'] =  attention_mask_[:nb_train], attention_mask_[nb_train:nb_train+nb_val], attention_mask_[-nb_test:]"
      ],
      "metadata": {
        "id": "I3KOWKdsfECj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {}\n",
        "loader = {}\n",
        "for split in ['train', 'val', 'test']:\n",
        "    datasets[split] =  Data.TensorDataset(input_ids[split], attention_mask[split], label[split])\n",
        "    loader[split] = Data.DataLoader(datasets[split], batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "hXiepYAjfHl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=bert_lr)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[30], gamma=0.1)"
      ],
      "metadata": {
        "id": "B33edrG9fMHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(engine, batch):\n",
        "    global model, optimizer\n",
        "    model.train()\n",
        "    model = model.to(gpu)\n",
        "    optimizer.zero_grad()\n",
        "    (input_ids, attention_mask, label) = [x.to(gpu) for x in batch]\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(input_ids, attention_mask)\n",
        "    y_true = label.type(th.long)\n",
        "    loss = F.cross_entropy(y_pred, y_true)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = loss.item()\n",
        "    with th.no_grad():\n",
        "        y_true = y_true.detach().cpu()\n",
        "        y_pred = y_pred.argmax(axis=1).detach().cpu()\n",
        "        train_acc = accuracy_score(y_true, y_pred)\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "kg6mdOTtfMPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Engine(train_step)"
      ],
      "metadata": {
        "id": "BK005wuAfd0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(engine, batch):\n",
        "    global model\n",
        "    with th.no_grad():\n",
        "        model.eval()\n",
        "        model = model.to(gpu)\n",
        "        (input_ids, attention_mask, label) = [x.to(gpu) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(input_ids, attention_mask)\n",
        "        y_true = label\n",
        "        return y_pred, y_true"
      ],
      "metadata": {
        "id": "gO3VDp6afgMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Engine(test_step)\n",
        "metrics={\n",
        "    'acc': Accuracy(),\n",
        "    'nll': Loss(th.nn.CrossEntropyLoss())\n",
        "}\n",
        "for n, f in metrics.items():\n",
        "    f.attach(evaluator, n)"
      ],
      "metadata": {
        "id": "gapw7rpmfizn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_training_results(trainer):\n",
        "    evaluator.run(loader['train'])\n",
        "    metrics = evaluator.state.metrics\n",
        "    train_acc, train_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    evaluator.run(loader['val'])\n",
        "    metrics = evaluator.state.metrics\n",
        "    val_acc, val_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    evaluator.run(loader['test'])\n",
        "    metrics = evaluator.state.metrics\n",
        "    test_acc, test_nll = metrics[\"acc\"], metrics[\"nll\"]\n",
        "    logger.info(\n",
        "        \"\\rEpoch: {}  Train acc: {:.4f} loss: {:.4f}  Val acc: {:.4f} loss: {:.4f}  Test acc: {:.4f} loss: {:.4f}\"\n",
        "        .format(trainer.state.epoch, train_acc, train_nll, val_acc, val_nll, test_acc, test_nll)\n",
        "    )\n",
        "    if val_acc > log_training_results.best_val_acc:\n",
        "        logger.info(\"New checkpoint\")\n",
        "        th.save(\n",
        "            {\n",
        "                'bert_model': model.bert_model.state_dict(),\n",
        "                'classifier': model.classifier.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': trainer.state.epoch,\n",
        "            },\n",
        "            os.path.join(\n",
        "                ckpt_dir, 'checkpoint.pth'\n",
        "            )\n",
        "        )\n",
        "        log_training_results.best_val_acc = val_acc\n",
        "    scheduler.step()\n",
        "\n",
        "        \n",
        "log_training_results.best_val_acc = 0\n",
        "trainer.run(loader['train'], max_epochs=nb_epochs)"
      ],
      "metadata": {
        "id": "1DbSCgM6lass"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}